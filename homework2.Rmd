---
title: 'Homework #2'
author: "Srikar Murali (#11593114)"
date: "February 22, 2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MVN)
library(Hotelling)
library(ICSNP)
```

## 1.

#### a.
$X_1$ and $X_2$ are independent since when looking at the variance-covariance matrix there is no correlation between the two variables.

#### b.
$X_1$ and $X_3$ are dependent since when looking at the variance-covariance matrix there is a correlation between the two. In fact they are negatively correlated (-1).

#### c.

```{r 1c}
x <- matrix(c(1,-2,0,-2, 5, 0, 0, 0, 2), nrow=3, ncol=3, byrow = TRUE)
m <- matrix(c(1, 0, 0, 1, -3, -2), nrow = 2, ncol = 3, byrow = TRUE)
ans <- m%*%x%*%t(m)
ans

```

They are not independent since the variance-covariance matrix value between $X_1$ and $X1 - 3X_2 - 2X_3$ is not 0, in fact it is 7.

## 2.


Marginal Distribution of $V_1$

$V_1 \sim N_{p}(\sum\limits_{i=1}^4 a_j\mu_j, (\sum\limits_{i=1}^4 (a_j)^2)\sum)$

$a_j = \{1/4, -1/4, 1/4, 0\}$

$\mu V_1 = \sum\limits_{i=1}^4 a_j\mu_j = 1/4 - 1/4 + 1/4 + 0 = 1/4\mu$

$Cov(V_1) = \sum\limits_{i=1}^4 (a_j)^2)\sum = 1/16 + 1/16 + 1/16 + 0 = 3/16\sum$

$V_1 \sim N_p(1/4\mu, 3/16\sum)$

Marginal Distribution of $V_2$

$V_2 \sim N_{p}(\sum\limits_{i=1}^4 b_j\mu_j, (\sum\limits_{i=1}^4 (b_j)^2)\sum)$

$a_j = \{1/4, 1/4, -1/4, -1/4\}$

$\mu V_2 = \sum\limits_{i=1}^4 b_j\mu_j = 1/4 + 1/4 - 1/4 - 1/4 = 0$

$Cov(V_2) = \sum\limits_{i=1}^4 (b_j)^2)\sum = 1/16 + 1/16 + 1/16 + 1/16 = 1/4\sum$

$V_2 \sim N_p(0, 1/4\sum)$

Joint Distribution of $V_1$ and $V_2$

```{r 2}
muV1V2 <- c(1/4, 0)
v1 <- c(1/4, -1/4, 1/4, 0)
v2 <- c(1/4, 1/4, -1/4, -1/4)
atb <- t(v1)%*%v2
atb

```

$[V_1, V_2]^T \sim N_{2p}(\mu^*, \sum^*)$

$\mu^*$ = $$\left(\begin{array}{cc} 1/4\mu\\0\end{array}\right)$$

$\sum^*$ = $$\left(\begin{array}{cc} 3/16\sum & -1/16\sum\\-1/16\sum & 1/4\sum \end{array}\right)$$


## 3.

Maximum likelihood estimate is the column mean $\bar{x}$. The covariance matrix is the estimatation of $\sum$ which is $\hat{\sum}$

$\hat{\sum} = 1/n\sum\limits_{j=1}^n (X_j - \bar{X})(X_j- \bar{X})'$

```{r 3}
x <- matrix(c(3,6,4,4,5,7,4,7), nrow = 4, ncol = 2, byrow=TRUE)
x
xbar <- colMeans(x)
xbar
one <- matrix(c(-1, 0), nrow=2, ncol=1)%*%matrix(c(-1, 0), nrow=1, ncol=2)
two <- matrix(c(0, -2), nrow=2, ncol=1)%*%matrix(c(0, -2), nrow=1, ncol=2)
three <- matrix(c(1, 1), nrow=2, ncol=1)%*%matrix(c(1, 1), nrow=1, ncol=2)
four <- matrix(c(0, 1), nrow=2, ncol=1)%*%matrix(c(0, 1), nrow=1, ncol=2)

sigmahat <- 1/4*(one + two + three + four)
sigmahat

```


## 4.

#### a.
The distribution of $\bar{X} \sim N_4(\mu, 1/60\sum)$

#### b.
The distribution of $n(\bar{X} - \mu)'\sum^{-1}(\bar{X} - \mu)$ can be approximated with a $\chi^2_4$ distribution with four degrees of freedom since $\bar{X} \sim N_4(\mu, 1/60\sum)$. $\bar{X}$ is distributed as a normal multivariate distribution, thus it is distributed as a chi-square distribution. 

#### c.
The distribution of $(\bar{X_1} - \mu)'\sum^{-1}(\bar{X_1} - \mu)$ can be approximated with a $\chi^2_4$ distribution with four degrees of freedom. This is the standard form of a chi-square distribution since $X_1$ is a normal random variable from a multivariate distribution.

#### d.
Since the sample size of 60 is far greater than the number of variables, 4, this distribution can be approximated by a chi-square distribution. In other words n-p is large. Thus, the distribution of $n(\bar{X} - \mu)'S^{-1}(\bar{X} - \mu)$ can be approximated with a $\chi^2_4$ with four degrees of freedom.


## 5.

```{r 5}
x <- c(20.6, 18.7, 14.2, -15.7, 7.7, -11.6, 8.8, 9.8, 19.2)
qqnorm(x)
qqline(x)
shapiro.test(x)
```

The data does not look normally distributed as the points do not lie on or near the normal line. Thus, from the Q-Q plot, the data is either barely normal or not normal. The Shapiro-Wilk test confirms that the data is normal; though the claim of normality is not too strong.


## 6.

#### a.

```{r 6a}
x <- matrix(c(3,5,5,7,7,7,8,9,10,11,2.3,1.9,1,0.7,0.3,1,1.05,0.45,0.70,0.30), nrow=10, ncol=2)

xbar <- colMeans(x)

S <- cov(x)
invS <- solve(S)
statDist <- function(x) {
  n <- dim(x)[1]
  fin <- c()
  
  xbar <- colMeans(x)
  invS <- solve(cov(x))
  for(i in 1:n) {
    dist <- t(x[i,] - xbar)%*%invS%*%(x[i,]-xbar)
    fin[i] <- dist
  }
  return(fin)
}
vals <- statDist(x)
vals

```


#### b.

```{r 6b}

dist <- sort(vals)

qlev <- c()
for (j in 1:10) {
  qlev[j] <- (j-0.5)/10
}
qlev
smean <- mean(vals)
ssd <- sd(vals)
norm_quantiles <- qchisq(qlev, df=2)
norm_quantiles
plot(norm_quantiles, dist, xlab = 'Theoretical Quantiles', ylab = 'Sample Quantiles', main = 'Chi-Square Q-Q plot')
abline(a = 0, b =1)
mardiaTest(x, qqplot = TRUE)
```

Based on the Q-Q plot for the chi-square distribution using both the manual and MVN package; the data seems to roughly fall on the line, though the line touches very few of the data points. The line seems to reasonably fit the data, thus implying normality. The Mardia test confirms this claim by also stating multivariate normality.

## 7.

```{r 7}
data <- read.csv('hw2_data.csv')

dom_rad <- qqnorm(data$Dominant_radius)
qqline(data$Dominant_radius)

rad <- qqnorm(data$Radius)
qqline(data$Radius)

dom_hum <- qqnorm(data$Dominant_humerus)
qqline(data$Dominant_humerus)

hum <- qqnorm(data$Humerus)
qqline(data$Humerus)

dom_ulna <- qqnorm(data$Dominant_ulna)
qqline(data$Dominant_ulna)

ulna <- qqnorm(data$Ulna)
qqline(data$Ulna)
sdist <- sort(statDist(as.matrix(data)))

qlev2 <- c()
for (j in 1:25) {
  qlev2[j] <- (j-0.5)/25
}


norm_quantiles2 <- qchisq(qlev2, df=6)
norm_quantiles2
plot(norm_quantiles2, sdist, xlab = 'Theoretical Quantiles', ylab = 'Sample Quantiles', main = 'Chi-Square Q-Q plot')
abline(a = 0, b =1)
mardiaTest(data, qqplot = TRUE)
```


The data is approximately normal, the qqplot computed manually and the Mardia test reveal that the data is approximately normal in the multivariate setting. In addition each of the components seems to be normal as well.



## 8.


#### a.

$H_0: \mu = 0$

$H_a: \mu \neq 0$

```{r 8a}
ztest <- function(x, mu0, alpha, sigma) {
  xbar <- mean(x)
  n <- length(x)
  zstat <- sqrt(n)*(xbar-mu0)/sigma
  critical_value <- qnorm(1-(alpha/2), 0, 1)
  y <- c("Do not reject")
  if(abs(zstat) > critical_value) {
    y <- c("Reject H0")
  }
  return(y)
}
x <- c(0.4, -0.2, 0.4, 0.0, 0.8, 1.8, 1.3, 0.1, -0.1, 0.4)
ztest(x, 0, 0.05, 0.75)

```

The Z-test using the population standard deviation states to reject the null hypothesis, thus the mean is not 0.

#### b.

$H_0: \mu = 0$

$H_a: \mu \neq 0$

```{r 8b}


xbar <- mean(x)
mu0 <- 0
s <- sd(x)
n <- length(x)
tstat <- sqrt(n)*(xbar-mu0)/s
tstat

alpha <- 0.05
n <- length(x)
df <- n - 1
critical_value <- qt(0.975, df)
critical_value

t.test(x, alternative = c("two.sided"), mu = 0, conf.level = 0.95)

```

Doing the t-test manually along with using the inbuilt function reveals similar results to the Z-test. We reject the null hypothesis, the mean is not 0 since the 
t-statistic is greater than the critical value and the p-value is less than 0.05.

## 9.

#### a.

$H_0: \mu = (0.3, 10)'$

$H_a: H_0$ is not true.

```{r 9a}
analysis <- matrix(c(0.48, 40.53, 2.19, 0.55, 0.74, 0.66, 0.93, 0.37, 0.22, 12.57, 73.68, 11.13, 20.03, 20.29, 0.78, 4.64, 0.43, 1.08), nrow=9, ncol = 2)

hottelingT2 <- function (x, mu0, alpha) {
  if(dim(x)[2] != length(mu0)) {
    stop("dimension mismatch")
  }
  xbar <- colMeans(x)
  invS <- solve(cov(x))
  tsq <- n*t(xbar-mu0)%*%invS%*%(xbar-mu0)
  p <- dim(x)[2]
  n <- dim(x)[1]
  fquantile <- qf(1-alpha,p, n-p)
  critical_value <- (((n-1)*p)/(n-1))*fquantile
  res <- c("Do no reject H0")
  if(tsq > critical_value) {
    res <- c('Reject H0')
  }
  return (res)
}

hottelingT2(analysis, mu0=c(0.3, 10), 0.05)

HotellingsT2(analysis, mu = c(0.3, 10), test ='f')
```

Doing the Hotelling's $T^2$ test both manually and using the ICSNP packages reveals that both tests fail to reject the null hypothesis. There isn't enough evidence to state that $\mu$ = (0.3, 10)'.

#### b.
```{r 9b}

x1 <- qqnorm(analysis[, 1])
qqline(analysis[1, ])

x2 <- qqnorm(analysis[, 2])
qqline(analysis[, 2])

analysisdist <- sort(statDist(analysis))
qlev3 <- c()
for (j in 1:9) {
  qlev3[j] <- (j-0.5)/9
}


norm_quantiles3 <- qchisq(qlev3, df=2)
norm_quantiles3
plot(norm_quantiles3, analysisdist, xlab = 'Theoretical Quantiles', ylab = 'Sample Quantiles', main = 'Chi-Square Q-Q plot')
abline(a = 0, b =1)
mardiaTest(analysis, qqplot = TRUE)

```



When assessing marginal normality there are several outliers and the data does not seem to fit a normal distribution as seen in the Q-Q plots. When assessing joint normality the data still has some outliers and does not seem to fit a normal distribution. Indeed when using Q-Q plots and the Mardia test the data is not normal.